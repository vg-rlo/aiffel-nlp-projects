{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smaller-barrier",
   "metadata": {},
   "source": [
    "# 10-1. 들어가며\n",
    "나름대로 혁신적이었던 Sequence-to-Sequence 구조로 번역기를 만들었으나 성능이 기대에 미치지 않아 실망하신 분도 계신가요? 예문 한두 개 빼고는 거의 다 이상한 번역문만 나와서 슬펐던 기억이 있으시더라도…😢 하지만 이번엔 다를 거예요!\n",
    "    \n",
    "트랜스포머는 현재까지도 각종 번역 부문에서 최고의 성능을 자랑하는 모델이니 이번에야말로 정말 멋진 번역기를 만들어 보실 수 있을 거예요. 게다가 이전 강의 노드에서 배웠던 신선한 개념들이 어떻게 구현이 될지 궁금하지 않나요? 트랜스포머 모델은 앞으로도 NLP 분야에서는 어느 곳에도 빠지지 않는 가장 중요한 모델구조의 근간이 되기 때문에, 오늘 구현 실습을 통해 트랜스포머의 구조를 꼼꼼이 파악해 봅시다.\n",
    "\n",
    "### 준비물\n",
    "터미널을 열고 프로젝트를 위한 디렉토리를 생성해 주세요.\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer\n",
    "```\n",
    "실습에서는 한국어가 포함된 말뭉치를 사용하므로, 한국어를 잘 시각화하기 위한 준비가 필요합니다. 다만 matplotlib 라이브러리의 기본 폰트는 한국어를 지원하지 않아요! 올바른 Attention Map을 확인하기 위해 한국어를 지원하는 폰트로 변경해 주도록 합시다.\n",
    "    \n",
    "아직 컴퓨터에 글꼴이 설치되어 있지 않다면, 터미널에서 나눔 글꼴을 설치합니다.\n",
    "    \n",
    "```\n",
    "$ sudo apt -qq -y install fonts-nanum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "described-christianity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-episode",
   "metadata": {},
   "source": [
    "# 10-2. 내부 모듈 구현하기\n",
    "이번 코스는 트랜스포머를 완성하는 데에 필요한 모듈들을 하나하나 만든 후, 조립하여 완성하는 방식으로 진행할 겁니다.\n",
    "    \n",
    "자, Tensor로 변환한 입력 데이터가 주어졌다고 가정하고 흐름을 생각해봅시다. 최초의 텍스트 입력 데이터는 [ batch_size x length ] 의 형태를 가지고 있겠죠? 번역이 끝나고 난 최종 출력 데이터는 [ batch_size x length x vocab_size ]의 형태를 가지게 됩니다. 번역 문제는 결국 매 스텝마다 vocab_size 만큼의 클래스 개수 중에 적당한 단어를 선택하는 작업을 length만큼 반복하는 것이니까요. 모델 구성하는 과정에서 레이어를 통과할 때마다 텐서의 shape가 어떻게 바뀌는지를 눈여겨 봅시다.    \n",
    "\n",
    "1. 입력 데이터 → [ batch_size x length ]\n",
    "2. Source & Target Embedding → [ batch_size x length x d_emb ]\n",
    "3. Positional Encoding 강의 노드에서 구현을 했었죠? 2번의 결과에 더해지므로 shape 변화는 없습니다.\n",
    "4. Multi-Head Attention 아래와 같이 여러 개의 서브 모듈들이 존재합니다.\n",
    "    1. Split Heads → [ batch_size x length x heads x (d_emb / n_heads) ]\n",
    "    2. Masking for Masked Attention\n",
    "    3. Scaled Dot Product Attention\n",
    "    4. Combine Heads →[ batch_size x length x d_emb ]\n",
    "5. Residual Connection\n",
    "6. Layer Normalization\n",
    "7. Position-wise Feed-Forward Network → [ batch_size x length x d_ff ]\n",
    "8. Output Linear Layer → [ batch_size x length x vocab_size ]    \n",
    "\n",
    "와… 정말 많네요… 그래도 굵게 표시된 모듈을 제외하면 나머지는 텐서플로우 내부에 이미 구현체가 포함되어 있어서 간단하게 사용할 수 있답니다! 게다가 차근차근 이해해나가면 하나도 어렵지 않은 내용들이예요. Positional Encoding부터 시작해보죠!\n",
    "    \n",
    "여느 때처럼 프로젝트에 사용될 라이브러리를 먼저 import 해주세요.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olive-placement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-suicide",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "![image](https://user-images.githubusercontent.com/69677950/115481614-f46cfc80-a287-11eb-8263-a7c0389a52aa.png)    \n",
    "가장 먼저 구현할 Positional Encoding은 다행히도 이전 강의 노드에서 구현해 본 경험이 있습니다! 소스를 빌려오면,    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unusual-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-reservoir",
   "metadata": {},
   "source": [
    "이렇게 생겼었죠! 가급적이면 이 소스를 그대로 사용하는 것이 여러분에게도 좋겠죠 ^_^? 이 소스는 건들지 않도록 하겠습니다. 다음, Multi-Head Attention으로!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-allocation",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "![image](https://user-images.githubusercontent.com/69677950/115481717-2e3e0300-a288-11eb-946a-9c17cc4cae81.png)    \n",
    "Multi-Head Attention은 여러 개의 서브 모듈을 결합하여 완성됩니다. Embedding된 입력을 Head 수로 분할하는 split_heads(), 분할된 입력으로부터 Attention 값을 구하는 scaled_dot_product_attention(), 연산이 종료되고 분할된 Head를 다시 하나로 결합시켜주는 combine_heads() 까지 MultiHeadAttention 클래스를 정의하여 모두 포함시켜줄 거예요!\n",
    "    \n",
    "뭔가 빠진 것 같다면 그것은 바로 Masking 부분! 마스크의 형태를 결정하는 것이 모델 외부의 훈련 데이터기 때문에 그를 생성하는 함수는 MultiHeadAttention 외부에 정의되는 것이 올바르겠죠! 마스크를 생성하는 함수는 모델을 완성한 후에 구현하도록 하겠습니다. 대신 생성된 마스크를 처리할 수 있도록 scaled_dot_product_attention() 에는 아래 한 줄을 포함하세요!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joined-joyce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c1515472f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scaled_qk: Attention을 위한 Softmax 직전의 Scaled QK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscaled_qk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "# scaled_qk: Attention을 위한 Softmax 직전의 Scaled QK\n",
    "\n",
    "if mask is not None: scaled_qk += (mask * -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "material-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "\n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "\n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "\n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-blake",
   "metadata": {},
   "source": [
    "음… 쉽지 않죠? 저는 Multi-Head Attention을 처음 구현할 적에 Head를 나눈다는 개념이 너무 헷갈려서 반복문을 돌며 [idx : idx + self.depth] 로 인덱싱해서 처리를 했던 기억이 있답니다… 이렇게 하면 연산하는 데 오조 오억 년 걸려요. 😢 부디 여러분은 같은 시행착오를 겪지 않길 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-jaguar",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network\n",
    "![image](https://user-images.githubusercontent.com/69677950/115481824-60e7fb80-a288-11eb-8eb0-6d64079aedd3.png)    \n",
    "Position-wise Feed-Forward Network는 논문 설명에서도 아주 간략하게 적혀있었죠? 구현도 아주 쉬운 편이랍니다. 바로 확인하시죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "three-share",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-heaven",
   "metadata": {},
   "source": [
    "d_ff 는 논문의 설명대로라면 2048 일 거고, d_model 은 512 겠죠? [ batch x length x d_model ] 의 입력을 받아 w_1 이 2048차원으로 매핑하고 활성함수 ReLU를 적용한 후, 다시 w_2 를 통해 512차원으로 되돌리는 과정까지! 이렇게 쉽게 FFN 완성입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-hollow",
   "metadata": {},
   "source": [
    "## 10-3. 모듈 조립하기\n",
    "여기까지 내부에 포함될 모듈들을 모두 완성하셨습니다, 대단해요! 이 모든 모듈들을 가지고 트랜스포머를 완성할 수 있는데, 정확하게는 트랜스포머의 Encoder 한 층과 Decoder 한 층을 각각 완성할 수 있습니다!\n",
    "   \n",
    "그럼 이만큼의 코드를 5번 더 짜야 여섯 층짜리 논문 속 트랜스포머가 완성되나요..?\n",
    "    \n",
    "이런 걱정을 하셨다면 안심하세요, 우리는 조금 더 멋지고 효율적인 방법으로 트랜스포머를 완성할 거니까요!     \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/69677950/115481914-807f2400-a288-11eb-8951-564bff250468.png)\n",
    "    \n",
    "강의 노드가 아닌데 갑자기 표? 뜬금없지만 <Attention Is All You Need> 논문에 포함된 이 표는 트랜스포머가 얼마나 많은 실험을 통해서 탄생한 모델인지 보여줍니다. 이런 실험이 가능하게 하려면 모델이 동적으로 완성될 수 있게끔 해야 해요. 즉, 레이어 수를 원하는 만큼 쌓아 실험을 자유자재로 할 수 있게 모델을 완성하자는 거죠!\n",
    "    \n",
    "방법은 단순합니다, 마치 텐서플로우의 Dense 레이어를 사용하듯이 EncoderLayer, DecoderLayer를 쓸 수 있게 tf.keras.layers.Layer 클래스를 상속받아 레이어 클래스로 정의해 주면 돼요. 여러분은 이미 Layer 클래스를 정의해본 적이 있는데, 바로 직전의 MultiHeadAttention 이 그렇게 정의된 레이어랍니다! 이 방법을 사용하면 아래와 같은 용법으로 트랜스포머 레이어를 사용할 수 있죠.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-fancy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerEncoderLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ebf3c749c2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 10개의 Encoder Layer도 한 방에!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menc_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-4ebf3c749c2c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 10개의 Encoder Layer도 한 방에!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menc_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerEncoderLayer' is not defined"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "# 10개의 Linear Layer를 한 방에!\n",
    "linear_layers = [tf.keras.layers.Dense(30) for _ in range(N)]\n",
    "\n",
    "# 10개의 Encoder Layer도 한 방에!\n",
    "enc_layers = [TransformerEncoderLayer(30) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-recovery",
   "metadata": {},
   "source": [
    "멋지지 않나요? 혹시라도 이런 동적인 방식이 낯설다면 지금부터라도 익숙해지시길 강력하게 권장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-sperm",
   "metadata": {},
   "source": [
    "## Encoder 레이어 구현하기\n",
    "그럼 본격적으로 레이어를 디자인해보죠! 먼저 EncoderLayer 구현을 예시로 보여드릴게요. 이를 참고하여 DecoderLayer 를 구현하시면 됩니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cheap-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-nursing",
   "metadata": {},
   "source": [
    "Transformer의 구현은 정말 많은데, 그중에서 Normalization Layer의 위치에 대한 논의가 종종 나온답니다. 실제 논문에서는 [ Input ] - [ Module ] - [ Residual ] - [ Norm ] (Module = MHA, FFN)으로 표현되어 있지만 정작 Official 구현인 구글의 Tensor2Tensor 에서는 [ Input ] - [ Norm ] - [ Module ] - [ Residual ] 방식을 사용했어요.\n",
    "    \n",
    "필자의 경험에 따르면 레이어가 많아질수록 후자가 약간 더 좋은 성능을 보였기에 필자는 논문 대신 Official 구현을 따르길 권장합니다! 이번 프로젝트는 소규모라서 큰 차이가 나지 않으니 알아두기만 해도 괜찮아요. 😃    \n",
    "    \n",
    "(참고) 트랜스포머의 Layer Normalization의 위치에 대한 논의를 다룬 On Layer Normalization in the Transformer Architecture이라는 제목의 논문이 2020년 초반에 발표되었습니다. 이 논문에서는 모듈 앞에 Normalization Layer를 두는 pre-LN 방식이 왜 유리한지를 설명하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-light",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현하기\n",
    "위 EncoderLayer 클래스를 참고하여 DecoderLayer 클래스를 완성하세요!\n",
    "     \n",
    "(참고: Decoder에서는 두 번의 Attention이 진행되니 반환되는 Attention도 두 개겠죠?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advanced-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-trustee",
   "metadata": {},
   "source": [
    "EncodeLayer 와 DecoderLayer 를 모두 정의했으니 이를 조립하는 것은 어렵지 않겠죠? 이를 이용해 Encoder와 Decoder 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "partial-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bigger-insulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-tribune",
   "metadata": {},
   "source": [
    "## Transformer 완성하기\n",
    "정의된 Encoder 와 Decoder 를 가지고 최종적으로 트랜스포머를 완성합니다!\n",
    "    \n",
    "아래 조건을 만족하며 소스의 빈칸을 채워 Transformer 클래스를 완성하세요!\n",
    "    \n",
    "조건    \n",
    "1. shared 변수를 매개변수로 받아 True 일 경우 Decoder Embedding과 출력층 Linear의 Weight를 공유할 수 있게 하세요! Weight가 공유될 경우 Embedding 값에 sqrt(d_model)을 곱해줘야 하는 것, 잊지 않으셨죠? (참고: tf.keras.layers.Layer.set_weights())\n",
    "2. 우리가 정의한 positional_encoding 의 반환값 형태는 [ Length x d_model ] 인데, 이를 더해 줄 Embedding 값 형태가 [ Batch x Length x d_model ] 이라서 연산이 불가능합니다. 연산이 가능하도록 수정하세요! (참고: tf.expand_dims(), np.newaxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "governmental-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-measurement",
   "metadata": {},
   "source": [
    "## 10-4. 모델 밖의 조력자들\n",
    "아까부터 은근하게 마음 한구석을 차지하고 있던 Masking을 살펴볼 시간이 다가왔습니다. 그리고 트랜스포머의 Learning Rate가 일반적이지 않다는 것도 기억하고 계실 거예요! 지금부터는 모델 외적인 부분을 정의해 주도록 하겠습니다. 이번 스텝에서는 데이터의 특성이나 학습 과정에 따라 달라지는 부분을 다루게 됩니다.   \n",
    "\n",
    "먼저 Masking입니다. 이전 노드에서 배운 generate_causality_mask() 를 그대로 사용하면 되는데, 약간 추가할 내용이 있습니다! 아래 구현을 먼저 보실까요?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mounted-running",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-james",
   "metadata": {},
   "source": [
    "generate_padding_mask() 는 Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할을 합니다. 일전에 Sequence-to-Sequence 모델에서 Loss에 대한 Masking을 해줄 때도 위와 같은 방법으로 진행했죠? 한 배치의 데이터에서 <PAD> 토큰으로 이뤄진 부분을 모두 찾아내는 마스크를 생성합니다. 눈으로 직접 확인해보죠!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "authentic-listing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 208,
       "width": 426
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-compact",
   "metadata": {},
   "source": [
    "첫 번째 마스크는 각 배치 별로 데이터의 꼬리 부분을 Masking 하는 형태임을 알 수 있습니다. 낯선 부분은 두 번째와 세 번째의 Decoder가 연관된 마스크인데… 이것이 바로 Causality Mask와 Padding Mask를 결합한 형태입니다! 자기 회귀적인 특성을 살리기 위해 Masked Multi-Head Attention에서 인과 관계 마스킹을 했던 것을 기억하시죠? 인과 관계를 가리는 것도 중요하지만 Decoder 역시 <PAD> 토큰은 피해 가야 하기 때문에 이런 형태의 마스크가 사용된답니다!\n",
    "    \n",
    "또, 트랜스포머는 고정된 Learning Rate를 사용하지 않았었죠! 논문의 해당 부분을 Optimizer까지 포함하여 다시 한번 살펴봅시다. 이전 노드에서 Learning Rate를 numpy 로 간단히 구현을 했었는데, 이번엔 Tensorflow 상에서 잘 구동될 수 있도록 LearningRateSchedule 클래스를 상속받아 구현해보죠!   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "decreased-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-memory",
   "metadata": {},
   "source": [
    "트랜스포머가 제안한 수식이 아니더라도 가변적인 Learning Rate를 사용하려면 위와 같이 구현을 하시면 됩니다. Optimizer와 Scheduler를 연결하는 과정도 아주 간단하죠! Optimizer는 논문에 정의된 대로 Adam Optimizer를 사용하며 세부 파라미터도 동일하게 맞춰 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-problem",
   "metadata": {},
   "source": [
    "## 10-5. 프로젝트: 더 멋진 번역기 만들기\n",
    "\n",
    "### Step 1. 데이터 다운로드 (로컬 유저용)\n",
    "아래 링크에서 korean-english-park.train.tar.gz 를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "\n",
    "* [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)\n",
    "💡이전 [ Seq2seq으로 번역기 만들기 ] 코스에서 사용한 데이터와 동일한 데이터입니다!\n",
    "\n",
    "터미널을 열어서 하단의 명령어를 입력해주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cloudy-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-21 10:06:15--  https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
      "Resolving github.com (github.com)... 52.78.231.108\n",
      "Connecting to github.com (github.com)|52.78.231.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz [following]\n",
      "--2021-04-21 10:06:15--  https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8718893 (8.3M) [application/octet-stream]\n",
      "Saving to: ‘korean-english-park.train.tar.gz’\n",
      "\n",
      "korean-english-park 100%[===================>]   8.31M  7.00MB/s    in 1.2s    \n",
      "\n",
      "2021-04-21 10:06:17 (7.00 MB/s) - ‘korean-english-park.train.tar.gz’ saved [8718893/8718893]\n",
      "\n",
      "korean-english-park.train.en\n",
      "korean-english-park.train.ko\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/aiffel/transformer/data\n",
    "!cd ~/aiffel/transformer/data\n",
    "\n",
    "!wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
    "!gzip -d korean-english-park.train.tar.gz\n",
    "!tar -xvf korean-english-park.train.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-memphis",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드 (클라우드 유저용)\n",
    "☁️클라우드를 이용중이신 분은 우측하단의 Cloud shell을 열어주세요.\n",
    "아래와 같이 공유디렉토리에 저장된 데이터를 가리키는 심볼릭 링크를 생성해 주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "provincial-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ~/data ~/aiffel/transformer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-startup",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 정제 및 토큰화\n",
    "1) set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "laden-wings",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aiffel-dj19/aiffel/transformer/data/korean-english-park.train.ko'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-837e0548f5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcleaned_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkor_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-837e0548f5a7>\u001b[0m in \u001b[0;36mclean_corpus\u001b[0;34m(kor_path, eng_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 데이터 정제 및 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkor_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkor_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aiffel-dj19/aiffel/transformer/data/korean-english-park.train.ko'"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-microwave",
   "metadata": {},
   "source": [
    "2) 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    "\n",
    "조건\n",
    "- 모든 입력을 소문자로 변환합니다.\n",
    "- 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "- 문장부호 양옆에 공백을 추가합니다.\n",
    "- 문장 앞뒤의 불필요한 공백을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "rubber-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-things",
   "metadata": {},
   "source": [
    "3) 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행합니다! 토큰화에는 Sentencepiece를 활용하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 generate_tokenizer() 함수를 정의합니다. 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻으세요. en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-broadcast",
   "metadata": {},
   "source": [
    "조건\n",
    "\n",
    "- 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    "- 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
    "- 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "   * \\<PAD> : 0 / \\<BOS> : 1 / \\<EOS> : 2 / \\<UNK> : 3\n",
    "- 참고: [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intense-fever",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-04b140dccf7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mkor_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-gardening",
   "metadata": {},
   "source": [
    "4) 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "passive-anchor",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-21-af603db8da9f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-af603db8da9f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-caution",
   "metadata": {},
   "source": [
    "### Step 3. 모델 설계\n",
    "오늘 배운 내용을 활용해서 Transformer 모델을 설계해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-booth",
   "metadata": {},
   "source": [
    "### Step 4. 훈련하기\n",
    "앞서 필요한 것들을 모두 정의했기 때문에 우리는 훈련만 하면 됩니다! 아래 과정을 차근차근 따라가며 모델을 훈련하고, 예문에 대한 멋진 번역을 제출하세요!\n",
    "\n",
    "1. 2 Layer를 가지는 Transformer를 선언하세요.\n",
    "    (하이퍼파라미터는 자유롭게 조절합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "present-canada",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-676f2f05b7e2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-676f2f05b7e2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    transformer = # [[YOUR CODE]]\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "transformer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-dollar",
   "metadata": {},
   "source": [
    "2. 논문에서 사용한 것과 동일한 Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언하세요. (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comparative-provincial",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-b5ef882c6e14>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-b5ef882c6e14>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    learning_rate = # [[YOUR CODE]]\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "learning_rate = # [[YOUR CODE]]\n",
    "optimizer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-plaintiff",
   "metadata": {},
   "source": [
    "3. Loss 함수를 정의하세요.\n",
    "Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, Masking 되지 않은 입력의 개수로 Scaling하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "infrared-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-accessory",
   "metadata": {},
   "source": [
    "4. train_step 함수를 정의하세요. 입력 데이터에 알맞은 Mask를 생성하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thousand-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-pastor",
   "metadata": {},
   "source": [
    "5. 학습을 진행합니다. 매 Epoch 마다 제시된 예문에 대한 번역을 생성하고, 멋진 번역이 생성되면 그때의 하이퍼파라미터와 생성된 번역을 제출하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-detector",
   "metadata": {},
   "source": [
    "예문\n",
    "1. 오바마는 대통령이다.\n",
    "2. 시민들은 도시 속에 산다.\n",
    "3. 커피는 필요 없다.\n",
    "4. 일곱 명의 사망자가 발생했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-algebra",
   "metadata": {},
   "source": [
    "결과(output)\n",
    "Translations\n",
    "> 1. obama is the president elect .\n",
    "> 2. they are in the city .\n",
    "> 3. they don t need to be a lot of drink .\n",
    "> 4. seven other people have been killed in the attacks .\n",
    "\n",
    "Hyperparameters\n",
    "> n_layers: 2\n",
    "> d_model: 512\n",
    "> n_heads: 8\n",
    "> d_ff: 2048\n",
    "> dropout: 0.3\n",
    "\n",
    "Training Parameters\n",
    "> Warmup Steps: 4000\n",
    "> Batch Size: 64\n",
    "> Epoch At: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-concentrate",
   "metadata": {},
   "source": [
    "번역 생성에는 아래 소스를 사용하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cultural-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()\n",
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sapphire-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-longer",
   "metadata": {},
   "source": [
    "translate() 함수의 plot_attention 변수를 True 로 주면 번역 결과에 대한 Attention Map을 시각화 해볼 수 있습니다.   \n",
    "💡 이번 프로젝트에서 제시한 예문은 Seq2seq으로 번역기 만들기의 예문과 동일합니다. Seq2seq과 Transformer로 만든 두 번역기의 성능을 하이퍼파라미터를 조정 등 다양한 연구해보시면 학습에 도움이 되실 거예요!\n",
    "    \n",
    "마지막으로, 학습의 전 과정을 구현한 코드를 첨부합니다. 구현과정에 참고해 주세요. 수고하셨습니다!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "configured-spelling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a48bf425609f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0midx_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-bahrain",
   "metadata": {},
   "source": [
    "### 루브릭 \n",
    "아래의 기준을 바탕으로 프로젝트를 평가합니다.\n",
    "    \n",
    "|평가문항|상세기준|\n",
    "|-----|-----|\n",
    "| 1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다. | 데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.|\n",
    "| 2. Transformer 번역기 모델이 정상적으로 구동된다.| Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.|\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
